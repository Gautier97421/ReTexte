import os
import asyncio
import tempfile
import hashlib
import json
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional
import threading
from queue import Queue

import uvicorn
from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from faster_whisper import WhisperModel
import torch
print("CUDA disponible :", torch.cuda.is_available())
if torch.cuda.is_available():
    print("Nom du GPU :", torch.cuda.get_device_name(0))
else:
    print("GPU non d√©tect√©, utilisation CPU")
print("Version CUDA utilis√©e par torch :", torch.version.cuda)


# Configuration optimis√©e
MODEL_SIZE = "medium"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
COMPUTE_TYPE = "float16" if torch.cuda.is_available() else "int8"
CACHE_DIR = Path("./cache")
JOBS_DIR = Path("./jobs")
MAX_FILE_SIZE = 2 * 1024 * 1024 * 1024  # 2GB
LARGE_FILE_THRESHOLD = 50 * 1024 * 1024  # 50MB

# NOUVEAU: Gestion de la concurrence
MAX_CONCURRENT_TRANSCRIPTIONS = 1  # Une seule transcription √† la fois
transcription_queue = Queue()
active_transcriptions = 0
transcription_lock = threading.Lock()

print(f"üöÄ ReTexte - Mode R√©seau Local")
print(f"   üì± Mod√®le: {MODEL_SIZE}")
print(f"   üñ•Ô∏è  Device: {DEVICE}")
print(f"   üë• Utilisateurs simultan√©s: ‚ôæÔ∏è  (interface)")
print(f"   üéµ Transcriptions simultan√©es: {MAX_CONCURRENT_TRANSCRIPTIONS}")

app = FastAPI(
    title="ReTexte", 
    version="2.2.0",
    description="Serveur multi-utilisateurs avec file d'attente"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Variables globales
whisper_model = None
jobs_status = {}
stats = {
    "total_transcriptions": 0, 
    "cache_hits": 0,
    "sync_jobs": 0,
    "async_jobs": 0,
    "concurrent_users": 0,
    "queue_length": 0,
    "avg_processing_speed": 0
}

# Cr√©er les dossiers
CACHE_DIR.mkdir(exist_ok=True)
JOBS_DIR.mkdir(exist_ok=True)

def load_model():
    """Charge le mod√®le pr√©-t√©l√©charg√© (thread-safe)"""
    global whisper_model
    if whisper_model is None:
        with transcription_lock:
            if whisper_model is None:  # Double-check
                print(f"ü§ñ Chargement du mod√®le {MODEL_SIZE}...")
                start_time = time.time()
                
                whisper_model = WhisperModel(
                    MODEL_SIZE, 
                    device=DEVICE, 
                    compute_type=COMPUTE_TYPE,
                    cpu_threads=min(8, os.cpu_count() or 4),
                    num_workers=1
                )
                load_time = time.time() - start_time
                print(f"‚úÖ Mod√®le {MODEL_SIZE} charg√© en {load_time:.1f}s!")

def get_cache_path(file_hash: str) -> Path:
    return CACHE_DIR / f"{file_hash}.json"

def save_cache(file_hash: str, result: Dict[str, Any]):
    try:
        with open(get_cache_path(file_hash), 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur sauvegarde cache: {e}")

def load_cache(file_hash: str) -> Optional[Dict[str, Any]]:
    cache_path = get_cache_path(file_hash)
    if cache_path.exists():
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                stats["cache_hits"] += 1
                return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lecture cache: {e}")
    return None

def transcribe_file_safe(file_path: str, language: str, filename: str, user_id: str = "unknown", job_id: Optional[str] = None) -> Dict[str, Any]:
    """Transcription thread-safe avec file d'attente et mise √† jour de la progression"""
    global active_transcriptions

    try:
        # Attendre son tour dans la file
        with transcription_lock:
            active_transcriptions += 1
            queue_position = active_transcriptions

        if queue_position > 1:
            print(f"‚è≥ Utilisateur {user_id}: En attente (position {queue_position} dans la file)")

        # Attendre que ce soit notre tour
        while True:
            with transcription_lock:
                if active_transcriptions == 1:  # C'est notre tour
                    break
            time.sleep(1)  # Attendre 1 seconde

        print(f"üéµ Utilisateur {user_id}: D√©but transcription de {filename}")

        # Chargement du mod√®le
        load_model()

        # V√©rification du fichier
        if not os.path.exists(file_path):
            raise Exception(f"Fichier non trouv√©: {file_path}")

        file_size = os.path.getsize(file_path)
        print(f"üìÅ Utilisateur {user_id}: Fichier {file_size} bytes")

        # Transcription
        start_time = time.time()

        segments_gen, info = whisper_model.transcribe(
            file_path,
            language=language if language != "auto" else None,
            beam_size=3,
            temperature=0.0,
            vad_filter=True,
            vad_parameters=dict(
                min_silence_duration_ms=500,
                speech_pad_ms=200
            ),
            chunk_length=30,
            condition_on_previous_text=False
        )

        # Construction du r√©sultat
        segments = list(segments_gen)
        segments_list = []
        full_text = ""
        total_segments = len(segments)

        for idx, segment in enumerate(segments):
            segment_data = {
                "start": segment.start,
                "end": segment.end,
                "text": segment.text.strip()
            }
            segments_list.append(segment_data)
            full_text += segment.text.strip() + " "

            # Mise √† jour de la progression (tous les 10 %)
            if job_id and total_segments > 10 and idx % max(1, (total_segments // 10)) == 0:
                progress_value = 20 + int((idx / total_segments) * 70)
                jobs_status[job_id]["progress"] = progress_value

        processing_time = time.time() - start_time
        file_size_mb = file_size / (1024 * 1024)
        speed_mb_per_min = (file_size_mb / processing_time) * 60 if processing_time > 0 else 0

        result = {
            "text": full_text.strip(),
            "segments": segments_list,
            "info": {
                "language": info.language,
                "duration": info.duration,
                "processing_time": processing_time,
                "speed_ratio": info.duration / processing_time if processing_time > 0 else 0,
                "total_segments": len(segments_list),
                "processing_speed_mb_per_min": speed_mb_per_min
            },
            "metadata": {
                "filename": filename,
                "model": MODEL_SIZE,
                "device": DEVICE,
                "processing_mode": "network",
                "file_size_mb": file_size_mb,
                "user_id": user_id,
                "queue_position": queue_position
            }
        }

        print(f"‚úÖ Utilisateur {user_id}: Transcription termin√©e en {processing_time:.1f}s")
        return result

    except Exception as e:
        print(f"‚ùå Utilisateur {user_id}: Erreur transcription: {str(e)}")
        raise e

    finally:
        # Lib√©rer la file d'attente
        with transcription_lock:
            active_transcriptions -= 1
            stats["queue_length"] = active_transcriptions


async def process_transcription_async(job_id: str, file_content: bytes, filename: str, language: str, user_id: str):
    """Traitement asynchrone avec file d'attente"""
    try:
        jobs_status[job_id] = {"status": "queued", "progress": 0, "user_id": user_id}
        
        # V√©rifier le cache
        file_hash = hashlib.sha256(file_content).hexdigest()
        cached_result = load_cache(file_hash)
        if cached_result:
            print(f"üìã Utilisateur {user_id}: Cache hit pour {filename}")
            jobs_status[job_id] = {"status": "completed", "result": cached_result, "progress": 100}
            return

        jobs_status[job_id]["status"] = "processing"
        jobs_status[job_id]["progress"] = 10
        
        # Traitement avec file d'attente
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(filename).suffix) as tmp_file:
            tmp_file.write(file_content)
            tmp_file_path = tmp_file.name

        try:
            jobs_status[job_id]["progress"] = 20
            result = transcribe_file_safe(tmp_file_path, language, filename, user_id)
            result["metadata"]["processing_mode"] = "async"
            
            jobs_status[job_id]["progress"] = 90
            save_cache(file_hash, result)
            stats["total_transcriptions"] += 1
            stats["async_jobs"] += 1
            
            jobs_status[job_id] = {"status": "completed", "result": result, "progress": 100}
            
        finally:
            try:
                os.unlink(tmp_file_path)
            except:
                pass
                
    except Exception as e:
        print(f"‚ùå Utilisateur {user_id}: Erreur async: {e}")
        jobs_status[job_id] = {"status": "error", "error": str(e)}

@app.get("/")
async def root():
    return {
        "message": "ReTexte - R√©seau Local Multi-Utilisateurs", 
        "status": "ok",
        "version": "2.2.0",
        "model": MODEL_SIZE,
        "device": DEVICE,
        "model_loaded": whisper_model is not None,
        "concurrent_support": True,
        "max_concurrent_transcriptions": MAX_CONCURRENT_TRANSCRIPTIONS,
        "current_queue_length": active_transcriptions
    }

@app.get("/health")
async def health():
    return {
        "status": "ok",
        "model": MODEL_SIZE,
        "device": DEVICE,
        "stats": stats,
        "active_jobs": len([j for j in jobs_status.values() if j["status"] == "processing"]),
        "model_loaded": whisper_model is not None,
        "queue_length": active_transcriptions,
        "concurrent_users": stats.get("concurrent_users", 0)
    }

@app.post("/transcribe")
async def transcribe_unified(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...), 
    language: str = "fr"
):
    """Transcription multi-utilisateurs avec file d'attente"""
    
    if not file.filename:
        raise HTTPException(status_code=400, detail="Nom de fichier manquant")
    
    # G√©n√©rer un ID utilisateur unique
    user_id = str(uuid.uuid4())[:8]
    
    file_content = await file.read()
    file_size = len(file_content)
    file_size_mb = file_size / (1024 * 1024)
    
    if file_size > MAX_FILE_SIZE:
        raise HTTPException(status_code=400, detail=f"Fichier trop volumineux")
    
    print(f"üìÅ Utilisateur {user_id}: Fichier re√ßu {file.filename} ({file_size_mb:.1f}MB)")
    
    # V√©rifier le cache
    file_hash = hashlib.sha256(file_content).hexdigest()
    cached_result = load_cache(file_hash)
    if cached_result:
        print(f"üìã Utilisateur {user_id}: R√©sultat en cache")
        return JSONResponse(content=cached_result)
    
    # Informer sur la file d'attente
    current_queue = active_transcriptions
    if current_queue > 0:
        print(f"‚è≥ Utilisateur {user_id}: {current_queue} transcription(s) en cours")
    
    # D√©cision du mode
    is_large_file = file_size > LARGE_FILE_THRESHOLD
    
    if is_large_file:
        # Mode asynchrone avec file d'attente
        job_id = str(uuid.uuid4())
        estimated_minutes = max(1, int(file_size_mb / 8))
        
        # Ajouter le temps d'attente si file d'attente
        if current_queue > 0:
            estimated_minutes += current_queue * 2  # +2min par job en attente
        
        print(f"üêò Utilisateur {user_id}: Job asynchrone {job_id}")
        
        background_tasks.add_task(process_transcription_async, job_id, file_content, file.filename, language, user_id)
        
        return {
            "job_id": job_id, 
            "status": "queued", 
            "estimated_time_minutes": estimated_minutes,
            "queue_position": current_queue + 1,
            "mode": "async",
            "user_id": user_id
        }
    
    else:
        # Mode synchrone avec file d'attente
        print(f"üêÅ Utilisateur {user_id}: Traitement synchrone")
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as tmp_file:
            tmp_file.write(file_content)
            tmp_file_path = tmp_file.name
    
        try:
            result = transcribe_file_safe(tmp_file_path, language, file.filename, user_id)
            result["metadata"]["processing_mode"] = "sync"
        
            save_cache(file_hash, result)
            stats["total_transcriptions"] += 1
            stats["sync_jobs"] += 1
        
            return JSONResponse(content=result)
        
        except Exception as e:
            error_msg = f"Erreur transcription: {str(e)}"
            print(f"‚ùå Utilisateur {user_id}: {error_msg}")
            raise HTTPException(status_code=500, detail=error_msg)
    
        finally:
            try:
                if os.path.exists(tmp_file_path):
                    os.unlink(tmp_file_path)
            except Exception as cleanup_error:
                print(f"‚ö†Ô∏è Erreur nettoyage: {cleanup_error}")

@app.get("/transcribe/status/{job_id}")
async def get_job_status(job_id: str):
    if job_id not in jobs_status:
        raise HTTPException(status_code=404, detail="Job non trouv√©")
    
    status = jobs_status[job_id].copy()
    status["current_queue_length"] = active_transcriptions
    return status

@app.get("/transcribe/result/{job_id}")
async def get_job_result(job_id: str):
    if job_id not in jobs_status:
        raise HTTPException(status_code=404, detail="Job non trouv√©")
    
    job = jobs_status[job_id]
    if job["status"] != "completed":
        raise HTTPException(status_code=400, detail=f"Job pas encore termin√©")
    
    return job["result"]

@app.get("/queue/status")
async def get_queue_status():
    """Statut de la file d'attente pour tous les utilisateurs"""
    return {
        "active_transcriptions": active_transcriptions,
        "total_jobs": len(jobs_status),
        "processing_jobs": len([j for j in jobs_status.values() if j["status"] == "processing"]),
        "queued_jobs": len([j for j in jobs_status.values() if j["status"] == "queued"]),
        "model_loaded": whisper_model is not None
    }

if __name__ == "__main__":
    port = int(os.getenv("PORT", 10001))
    print("üöÄ D√©marrage ReTexte - R√©seau Local...")
    print("   üë• Support multi-utilisateurs avec file d'attente")
    print("   üéµ Une transcription √† la fois (√©vite les conflits)")
    print()
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")
